{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "520c3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_reader(file):\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    my_dict={}\n",
    "    loaded_json=json.load(open(file))\n",
    "    doc_list=loaded_json[\"All questions\"]\n",
    "    for i in range(len(doc_list)):\n",
    "        my_dict[doc_list[i]['url']]=doc_list[i]['title']+\" \"+doc_list[i]['description']\n",
    "    return my_dict,doc_list\n",
    "\n",
    "def remove_nonwords(text):\n",
    "    import re\n",
    "    non_words = re.compile(r\"[^a-z']\")\n",
    "    processed_text = re.sub(non_words, ' ', text)\n",
    "    return processed_text.strip()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    from nltk.corpus import stopwords\n",
    "    stopwrds=stopwords.words('english')\n",
    "    words = [word for word in text.split() if word not in stopwrds]\n",
    "    return words\n",
    "\n",
    "def stem_words(words):\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return stemmed_words\n",
    "\n",
    "def preprocess_text(text):\n",
    "    processed_text = remove_nonwords(text.lower())\n",
    "    words = remove_stopwords(processed_text)\n",
    "    stemmed_words = stem_words(words)\n",
    "    return stemmed_words\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e4b17cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_creation(some_dict):\n",
    "    from collections import Counter\n",
    "    corpus=[]\n",
    "    for i in some_dict.keys():\n",
    "        words=preprocess_text(some_dict[i])\n",
    "        bag_of_words = Counter(words)\n",
    "        corpus.append(bag_of_words)\n",
    "    return corpus\n",
    "\n",
    "def compute_idf(corpus):\n",
    "    from collections import defaultdict\n",
    "    import math\n",
    "    num_docs = len(corpus)\n",
    "    idf = defaultdict(lambda: 0)\n",
    "    for doc in corpus:\n",
    "        for word in doc.keys():\n",
    "            idf[word] += 1\n",
    "\n",
    "    for word, value in idf.items():\n",
    "        idf[word] = math.log(num_docs / value)\n",
    "    return idf\n",
    "\n",
    "def compute_weights(idf,doc):\n",
    "    x=max(doc.values())\n",
    "    for word, value in doc.items():\n",
    "        doc[word] =  (doc[word]/x) * idf[word]\n",
    "    \n",
    "        \n",
    "def normalize(doc):\n",
    "    import math\n",
    "    denominator = math.sqrt(sum([e ** 2 for e in doc.values()]))\n",
    "    for word, value in doc.items():\n",
    "        doc[word] = value / denominator\n",
    "        \n",
    "def build_inverted_index(idf, corpus):\n",
    "    inverted_index = {}\n",
    "    for word, value in idf.items():\n",
    "        inverted_index[word] = {}\n",
    "        inverted_index[word]['idf'] = value\n",
    "        inverted_index[word]['postings_list'] = []\n",
    "\n",
    "    for index, doc in enumerate(corpus):\n",
    "        for word, value in doc.items():\n",
    "            inverted_index[word]['postings_list'].append([index, value])\n",
    "\n",
    "    return inverted_index\n",
    "\n",
    "def query_preprocess(text,inverted_index):\n",
    "    from collections import Counter\n",
    "    words_dictionary=set(inverted_index.keys())\n",
    "    query=preprocess_text(text)\n",
    "    query = [word for word in query if word in words_dictionary]\n",
    "    query = Counter(query)\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1ed4c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_dictionaries():\n",
    "    import nltk\n",
    "    from nltk.corpus import words\n",
    "    word_list = words.words()\n",
    "    english_dictionary={}\n",
    "    set_dictio=set([])\n",
    "    group_words={}\n",
    "    num_dictionary={}\n",
    "    for word in word_list:\n",
    "        english_dictionary[word.lower()]=word.lower()\n",
    "        num_dictionary[word]=len(word)\n",
    "    for word in word_list:\n",
    "        set_dictio.add(num_dictionary[word])\n",
    "    for i in set_dictio:\n",
    "        group=[]\n",
    "        for word in word_list:\n",
    "            if(int(i)==len(word)):\n",
    "                group.append(word.lower())\n",
    "        group_words[i]=group\n",
    "    return english_dictionary,group_words\n",
    "\n",
    "def minimum_distance_words(similar_words,word_to_check):\n",
    "    from Levenshtein import distance\n",
    "    distances=set([])\n",
    "    check={}\n",
    "    results=[]\n",
    "    for i,j in enumerate(similar_words):\n",
    "        edit_dist=distance(word_to_check,j)\n",
    "        check[j]=edit_dist\n",
    "        distances.add(edit_dist)\n",
    "    for key in check.keys():\n",
    "        if(check[key]==min(distances)):\n",
    "            results.append(key)\n",
    "    return results\n",
    "\n",
    "def auto_correct2(word,english_dict,word_groups):\n",
    "    preprocess_L=[]\n",
    "    words_to_check=[]\n",
    "    for i in range(len(word)-1,len(word)+2):\n",
    "        preprocess_L.extend(word_groups[str(i)])#str()\n",
    "    for checkword in preprocess_L:\n",
    "        if(word[0]==checkword[0]):\n",
    "            words_to_check.append(checkword)\n",
    "    results=minimum_distance_words(words_to_check,word)\n",
    "    if len(results)>1:\n",
    "        print(results)\n",
    "        print(\"Something is wrong with your input!\")\n",
    "        final_result=int(input(\"Please choose the index of one of the recomended words from the list or enter 0 to return the original word :\"))\n",
    "        if(final_result==0):\n",
    "            return word\n",
    "        else:\n",
    "            return results[final_result-1]\n",
    "    elif(len(results)==0):\n",
    "        return word\n",
    "    return results[0]\n",
    "\n",
    "def check_string(my_string,english_dict,word_groups):\n",
    "    my_string=remove_nonwords(my_string)\n",
    "    results=[]\n",
    "    for item in my_string.split():\n",
    "        try:\n",
    "            x=english_dict[item.lower()]\n",
    "        except:\n",
    "            x=auto_correct2(item.lower(),english_dict,word_groups)\n",
    "        results.append(x)\n",
    "    final=\" \".join(results)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8153dca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_1(path):\n",
    "    data,docs=data_reader(path)\n",
    "    corpus=corpus_creation(data)\n",
    "    idf=compute_idf(corpus)\n",
    "    for doc in corpus:\n",
    "        compute_weights(idf, doc)\n",
    "        normalize(doc)\n",
    "    inverted_index = build_inverted_index(idf, corpus)\n",
    "    return inverted_index,data,docs\n",
    "\n",
    "def main_2(inverted_index,data,docs):\n",
    "    import math    \n",
    "    import json\n",
    "\n",
    "    #english_dict,word_groups=english_dictionaries()\n",
    "    a_file = open(\"english_dictionary.json\", \"r\")\n",
    "    english_dict = json.load(a_file)\n",
    "    b_file = open(\"group_words.json\", \"r\")\n",
    "    word_groups = json.load(b_file)\n",
    "\n",
    "    \n",
    "\n",
    "    query=input(\"Give Query: \")\n",
    "\n",
    "    final_string=check_string(query,english_dict,word_groups)\n",
    "    print(final_string)\n",
    "    processed_q=query_preprocess(final_string,inverted_index)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    x=max(processed_q.values())\n",
    "\n",
    "    for word, value in processed_q.items():\n",
    "        processed_q[word] = inverted_index[word]['idf'] * ((processed_q[word]/x) )\n",
    "    normalize(processed_q)\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    from kneed import KneeLocator\n",
    "    vectorframe = pd.read_csv(\"vector.csv\" ,index_col=False)\n",
    "    vectorframe=vectorframe.drop(columns=['Unnamed: 0','level_0'])\n",
    "    data2 = vectorframe.fillna(0)\n",
    "    svd = TruncatedSVD(n_components =100)\n",
    "    svd.fit(data2)\n",
    "    transformed = svd.transform(data2)\n",
    "    svd_frame =pd.DataFrame(transformed)\n",
    "    #============================SVD_to_query=====================================================#\n",
    "    n = 10  # every 100th line = 1% of the lines\n",
    "    df1 = pd.read_csv('vector.csv', header=0, skiprows=lambda i: i % n != 0)\n",
    "    data3 = df1.fillna(0)\n",
    "    query_df=pd.DataFrame.from_dict(processed_q,orient='index')\n",
    "    query2=query_df.T\n",
    "    frames = [data3, query2]\n",
    "    result = pd.concat(frames)\n",
    "    result=result.fillna(0)\n",
    "    result\n",
    "\n",
    "    svd.fit(result)\n",
    "    transformed_query = svd.transform(result)\n",
    "    svd_query =pd.DataFrame(transformed_query)\n",
    "    svd_query=pd.DataFrame(transformed_query).iloc[-1]\n",
    "    kmeans_kwargs = {\"init\":\"k-means++\", \"n_init\":11, \"max_iter\": 300, \"random_state\":42}\n",
    "    sse =[]\n",
    "    for k in range(1,11):\n",
    "        kmeans = KMeans(n_clusters = k, **kmeans_kwargs)\n",
    "        kmeans.fit(svd_frame)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    kl = KneeLocator(range(1,11),sse, curve = 'convex', direction = \"decreasing\")\n",
    "\n",
    "    num = kl.elbow\n",
    "    kmeans = KMeans(n_clusters=14, init=\"k-means++\",max_iter=300,n_init = 11)\n",
    "    kmeans.fit(svd_frame)\n",
    "    lowestSSE = kmeans.inertia_\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    label = kmeans.fit_predict(svd_frame)\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    distance_matrix = cosine_similarity([svd_query],centroids)\n",
    "    u=np.argsort(distance_matrix)\n",
    "    u=u[0][-1]\n",
    "    distance_matrix2 = cosine_similarity([svd_query],svd_frame[label==u])\n",
    "    x=np.argsort(distance_matrix2[0])\n",
    "    x=x[-1]\n",
    "\n",
    "    docu=svd_frame[label==u].index\n",
    "    final=docu[x]\n",
    "    import json\n",
    "    file_dictionary=open('Num_dic_data.json',mode='r')\n",
    "    loaded_json=json.load(file_dictionary)\n",
    "    print(final)\n",
    "    return loaded_json[str(final)][1]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3fef21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"sample_100_without_duplicates.json\"\n",
    "inv_indx,data,docs=main_1(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8b7ffff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python label data point histogram\n",
      "202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"how to add custom metadata to pdf using node js I am reading a pdf file from AWS s3 bucket and want to generate a new file with additional custom metadata using node in lambda. I tried with pdf-lib NPM and was able to generate a new file but didn't find a way to add custom metadata. the NPM document only has methods to add default properties like title, author. Please suggest any way to add custom metadata site design / logo © 2021 Stack Exchange Inc; user contributions licensed under cc by-sa.                    rev\\xa02021.10.12.40432\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_2(inv_indx,data,docs)\n",
    "\n",
    "#results=main_2(inv_indx,data,docs)\n",
    "# Query1 : nrows alternative for read_sql_table\n",
    "#Query 2 : incompatible types: JSONLoader cannot be converted to Loader<JSONObject>\n",
    "#Query3: Laravel eager loading - one to many relation\n",
    "#Query 4 : python labelling new data points in a histogram\n",
    "#Query 5: matrices multiplication in mathematica"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
